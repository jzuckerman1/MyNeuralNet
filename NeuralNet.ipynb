{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "A significant proportion of this project was inspired by videos from 3Blue1Brown. Along with that, this will come heavily inspired from Neural Networks and Deep Learning, the free online book. It can be reached [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/). This version is outdated so many components were changed into how I conceptually understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"\n",
    "    Creates a Neural Network in specified layers with implemented Neural Network Methods\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes : list[int]):\n",
    "        \"\"\"\n",
    "        Initializes the Network with random, normally distributed, biases and weights.\n",
    "\n",
    "        Args:\n",
    "            sizes (list): List of the number of neurons in each layer\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "        self.weights = [np.random.randn(layer, prev) for prev, layer in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def feedForward(self, inputs : list[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Returns the output of the network given the input list\n",
    "\n",
    "        Args:\n",
    "            inputs (list): Input to the lyaer\n",
    "        \"\"\"\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            inputs = np.tanh(np.dot(weight, inputs) + bias)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None) -> None:\n",
    "        \"\"\"\n",
    "        Trains the network using Stoachastic Gradient Descent. \n",
    "\n",
    "        Args:\n",
    "            training_data (list of tuples): Training Data for the network\n",
    "            epochs (int): number of iterations to adjust network\n",
    "            mini_batch_size (int): size of each batch to pass through\n",
    "            learningRate (float): Amount to shift layers in backpropogration\n",
    "            test_data (list, optional): Passed through test data. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[j:j + mini_batch_size] for j in range(0, len(training_data), mini_batch_size)]\n",
    "            mini_batches = [batch for batch in mini_batches if len(batch) == mini_batch_size]\n",
    "            \n",
    "            for batch in mini_batches:\n",
    "                self.update_mini_batch(batch, learningRate)\n",
    "                \n",
    "            if test_data:\n",
    "                print(f'Epoch {i + 1}: {self.evaluate(test_data)} / {len(test_data)}') \n",
    "            else:\n",
    "                print(f'Epoch {i + 1} complete!')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_mini_batch(self, mini_batch : list[tuple[np.ndarray, np.ndarray]], learningRate : float):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using SGD and Back Propogation\n",
    "\n",
    "        Args:\n",
    "            mini_batch (list[tuple[np.ndarray, np.ndarray]]): list containing a subset of the training data\n",
    "            learningRate (float): Amount by which to shift existing layers after backpropogration\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            change_new_biases, change_new_weights = self.backprop(x, y)\n",
    "            new_biases = [cur + change for cur, change in zip(new_biases, change_new_biases)]\n",
    "            new_weights = [cur + change for cur, change in zip(new_weights, change_new_weights)]\n",
    "        \n",
    "        self.weights = [weight - (learningRate / len(mini_batch) * new_weight) for weight, new_weight in zip(self.weights, new_weights)]\n",
    "        self.biases = [bias - (learningRate / len(mini_batch) * new_biases) for bias, new_biases in zip(self.biases, new_biases)] \n",
    "        \n",
    "        \n",
    "    def backprop(self, x, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function. \n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array\n",
    "            y (np.ndarray): Expected output array\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #feed forward\n",
    "        activation = x\n",
    "        activations = [x] #To store all activations in each layer\n",
    "        zs = [] #To store all z vectors\n",
    "        \n",
    "        # Iterate across layers\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation).reshape(-1, 1) + bias # Have to reshape to correct dimensionality\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backwards pass\n",
    "        delta = self.cost_derivative(activations[-1].transpose()[0], y).reshape(-1, 1) * (1 / np.cosh(zs[-1])) ** 2 # Find the cost of our result (sech^2 from the derivative of tanh)\n",
    "        \n",
    "        new_biases[-1] = delta\n",
    "        new_weights[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for layer in range(-2, -len(self.sizes), -1): # Using negative indexing\n",
    "            z = zs[layer]\n",
    "            sp = np.tanh(z)\n",
    "            delta = np.dot(self.weights[layer + 1].transpose(), delta) * sp\n",
    "            new_biases[layer] = delta\n",
    "            if len(self.sizes) == 3: #Strange bug I encountered\n",
    "                new_weights[layer] = np.dot(delta, [activations[layer - 1]])\n",
    "            else:\n",
    "                new_weights[layer] = np.dot(delta, activations[layer - 1].T)\n",
    "            break\n",
    "            \n",
    "        return (new_biases, new_weights)\n",
    "    \n",
    "    def evaluate(self, test_data : list[tuple[np.ndarray, int]]):\n",
    "        \"\"\"\n",
    "        Return the number of correct outputs\n",
    "\n",
    "        Args:\n",
    "            test_data (list[tuple[np.ndarray, np.ndarray]]): test data in the form of ([input], output)\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedForward(x)), y) for (x,y) in test_data]\n",
    "        return sum(int(x + 0.01 > y and x - 0.01 < y) for (x, y) in test_results)\n",
    "        \n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return vector of partial derivatives of cost with respect to layer for the \n",
    "\n",
    "        Args:\n",
    "            output_activations (_type_): what we got\n",
    "            y (_type_): what the goal was\n",
    "        \"\"\"\n",
    "        return output_activations - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 81 / 100\n",
      "Epoch 2: 0 / 100\n",
      "Epoch 3: 0 / 100\n",
      "Epoch 4: 0 / 100\n",
      "Epoch 5: 0 / 100\n",
      "Epoch 6: 83 / 100\n",
      "Epoch 7: 83 / 100\n",
      "Epoch 8: 0 / 100\n",
      "Epoch 9: 0 / 100\n",
      "Epoch 10: 0 / 100\n"
     ]
    }
   ],
   "source": [
    "# Lets test the network!\n",
    "myNet = Network([2, 3, 3, 2]) # have 2 neurons, 3 neurons, 2 neurons.\n",
    "\n",
    "def createTestData(nums : tuple[int]):\n",
    "    if 1 in nums:\n",
    "        return [1, 0]\n",
    "    return [0, 1]\n",
    "\n",
    "myData = [(x, np.array(createTestData(x))) for x in [(np.array([int(10 * random.random()), int(10 * random.random())])) for _ in range(10000)]]\n",
    "testData = [(x, 1 if 1 in x else 0) for x in [(np.array([int(10 * random.random()), int(10 * random.random())])) for _ in range(100)]]\n",
    "\n",
    "myNet.SGD(myData, epochs=10, mini_batch_size=10, learningRate=2, test_data=testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 10\n",
      "Epoch 2: 0 / 10\n",
      "Epoch 3: 2 / 10\n",
      "Epoch 4: 0 / 10\n",
      "Epoch 5: 0 / 10\n",
      "Epoch 6: 0 / 10\n",
      "Epoch 7: 0 / 10\n",
      "Epoch 8: 0 / 10\n",
      "Epoch 9: 0 / 10\n",
      "Epoch 10: 0 / 10\n",
      "Epoch 11: 0 / 10\n",
      "Epoch 12: 0 / 10\n",
      "Epoch 13: 0 / 10\n",
      "Epoch 14: 1 / 10\n",
      "Epoch 15: 0 / 10\n",
      "Epoch 16: 2 / 10\n",
      "Epoch 17: 1 / 10\n",
      "Epoch 18: 2 / 10\n",
      "Epoch 19: 2 / 10\n",
      "Epoch 20: 1 / 10\n"
     ]
    }
   ],
   "source": [
    "# Lets test the network!\n",
    "myNet = Network([4, 12, 40, 12, 4]) # have 4 neurons, 4 neurons, 2 neurons.\n",
    "\n",
    "# Here is my rule: numbers must be in order. Output should be how many numbers are in order by the end. Simple O(n) \n",
    "def numsInPlace(nums : tuple[int]) -> int:\n",
    "    return sum([1 if i + 1 == nums[i] else 0 for i in range(len(nums))])\n",
    "\n",
    "assert(numsInPlace(tuple([1, 3, 2, 4])) == 2)\n",
    "\n",
    "myData = [(np.array(x), np.array([1 if numsInPlace(x) == i else 0 for i in range(4)])) for x in list(permutations([1, 2, 3, 4], 4))]\n",
    "testData = [(np.array(x), numsInPlace(x) - 1) for x in list(permutations([1, 2, 3, 4], 4)) if random.random() < (1/3)]\n",
    "# #self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None\n",
    "myNet.SGD(myData, epochs=20, mini_batch_size=10, learningRate=2, test_data=testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "It appears that this Neural Net doesn't effectively predict these patterns. I'm still curious about it's capabilities: thus, I plan to use it in some of my future projects. Perhaps using different inputs that are outside of just numbers would do a better job. The model was originally designed to predict handwritten numbers as according to the book. I'm sure it can be reworked using more inputs and an adjusted learning rate to be more accurate in application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
