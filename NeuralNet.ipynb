{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "A lot of this project was inspired by videos from 3Blue1Brown. Along with that, this will come heavily inspired from Neural Networks and Deep Learning, the free online book. It can be reached [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"\n",
    "    Creates a Neural Network in specified layers with implemented Neural Network Methods\n",
    "    \n",
    "    Args:\n",
    "        object : _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes : list[int]):\n",
    "        \"\"\"\n",
    "        Initializes the Network with random, normally distributed, biases and weights.\n",
    "\n",
    "        Args:\n",
    "            sizes (list): List of the number of neurons in each layer\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes) \n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "        self.weights = [np.random.randn(layer, prev) for prev, layer in zip(sizes[:-1], sizes[1:])] \n",
    "    \n",
    "    \n",
    "    \n",
    "    def feedForward(self, inputs : list[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Returns the output of the network given the input list\n",
    "\n",
    "        Args:\n",
    "            inputs (list): Input to the lyaer\n",
    "        \"\"\"\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            inputs = np.tanh(np.dot(weight, inputs) + bias)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learnRate : float, test_data : list[tuple[np.ndarray, int]] =None) -> None:\n",
    "        \"\"\"\n",
    "        Trains the network using Stoachastic Gradient Descent. \n",
    "\n",
    "        Args:\n",
    "            training_data (list of tuples): Training Data for the network\n",
    "            epochs (int): number of iterations to adjust network\n",
    "            mini_batch_size (int): size of each batch to pass through\n",
    "            learnRate (float): Amount to shift layers in backpropogration\n",
    "            test_data (list, optional): Passed through test data. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[j:j + mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]\n",
    "            \n",
    "            for batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, learnRate)\n",
    "                \n",
    "            if test_data:\n",
    "                print(f'Epoch {i}: {self.evaluate(test_data)} / {len(test_data)}') \n",
    "            else:\n",
    "                print(f'Epoch {i} complete!')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_mini_batch(self, mini_batch : list[tuple[np.ndarray, np.ndarray]], learnRate : float):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using SGD and Back Propogation\n",
    "\n",
    "        Args:\n",
    "            mini_batch (list[tuple[np.ndarray, np.ndarray]]): list containing a subset of the training data\n",
    "            learnRate (float): Amount by which to shift existing layers after backpropogration\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            change_new_biases, change_new_weights = self.backprop(x, y)\n",
    "            new_biases = [cur + change for cur, change in zip(new_biases, change_new_biases)]\n",
    "            new_weights = [cur + change for cur, change in zip(new_weights, change_new_weights)]\n",
    "        \n",
    "        self.weights = [weight - (learningRate / len(mini_batch) * new_weight) for weight, new_weight in zip(self.weights, new_weights)]\n",
    "        self.biases = [bias - (learningRate / len(mini_batch) * new_biases) for bias, new_biases in zip(self.biases, new_biases)]\n",
    "        \n",
    "        \n",
    "    def backprop(self, x, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function. \n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array\n",
    "            y (np.ndarray): Expected output array\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #feed forward\n",
    "        activation = x\n",
    "        activations = [x] #To store all activations in each layer\n",
    "        zs = [] #To store all z vectors\n",
    "        \n",
    "        # Iterate across layers\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backwards pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * (1 / cosh(zs[-1])) ** 2 # Find the cost of our result\n",
    "        new_bias[-1] = delta\n",
    "        new_weights[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for layer in range(self.num_layers - 2, 0, -1):\n",
    "            z = zs[layer]\n",
    "            sp = np.tanh(z)\n",
    "            delta = np.dot(self.weights[layer + 1].transpose(), delta) * sp\n",
    "            new_bias[layer] = delta\n",
    "            new_weights[layer] = np.dot(delta, activations[layer - 1].transpose())\n",
    "            \n",
    "        return (new_bias, new_weights)\n",
    "    \n",
    "    def evaluate(self, test_data : list[tuple[np.ndarray, int]]):\n",
    "        \"\"\"\n",
    "        Return the number of correct outputs\n",
    "\n",
    "        Args:\n",
    "            test_data (list[tuple[np.ndarray, np.ndarray]]): _description_\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return vector of partial derivatives of cost with respect to layer for the \n",
    "\n",
    "        Args:\n",
    "            output_activations (_type_): what we got\n",
    "            y (_type_): what the goal was\n",
    "        \"\"\"\n",
    "        return output_activations - y\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01935227886334212"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
