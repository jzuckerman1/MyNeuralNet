{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "A lot of this project was inspired by videos from 3Blue1Brown. Along with that, this will come heavily inspired from Neural Networks and Deep Learning, the free online book. It can be reached [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"\n",
    "    Creates a Neural Network in specified layers with implemented Neural Network Methods\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes : list[int]):\n",
    "        \"\"\"\n",
    "        Initializes the Network with random, normally distributed, biases and weights.\n",
    "\n",
    "        Args:\n",
    "            sizes (list): List of the number of neurons in each layer\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "        self.weights = [np.random.randn(layer, prev) for prev, layer in zip(sizes[:-1], sizes[1:])] \n",
    "    \n",
    "    \n",
    "    \n",
    "    def feedForward(self, inputs : list[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Returns the output of the network given the input list\n",
    "\n",
    "        Args:\n",
    "            inputs (list): Input to the lyaer\n",
    "        \"\"\"\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            inputs = np.tanh(np.dot(weight, inputs) + bias)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None) -> None:\n",
    "        \"\"\"\n",
    "        Trains the network using Stoachastic Gradient Descent. \n",
    "\n",
    "        Args:\n",
    "            training_data (list of tuples): Training Data for the network\n",
    "            epochs (int): number of iterations to adjust network\n",
    "            mini_batch_size (int): size of each batch to pass through\n",
    "            learningRate (float): Amount to shift layers in backpropogration\n",
    "            test_data (list, optional): Passed through test data. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[j:j + mini_batch_size] for j in range(0, len(training_data), mini_batch_size)]\n",
    "            \n",
    "            for batch in mini_batches:\n",
    "                self.update_mini_batch(batch, learningRate)\n",
    "                \n",
    "            if test_data:\n",
    "                print(f'Epoch {i}: {self.evaluate(test_data)} / {len(test_data)}') \n",
    "            else:\n",
    "                print(f'Epoch {i} complete!')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_mini_batch(self, mini_batch : list[tuple[np.ndarray, np.ndarray]], learningRate : float):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using SGD and Back Propogation\n",
    "\n",
    "        Args:\n",
    "            mini_batch (list[tuple[np.ndarray, np.ndarray]]): list containing a subset of the training data\n",
    "            learningRate (float): Amount by which to shift existing layers after backpropogration\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            change_new_biases, change_new_weights = self.backprop(x, y)\n",
    "            new_biases = [cur + change for cur, change in zip(new_biases, change_new_biases)]\n",
    "            new_weights = [cur + change for cur, change in zip(new_weights, change_new_weights)]\n",
    "        \n",
    "        self.weights = [weight - (learningRate / len(mini_batch) * new_weight) for weight, new_weight in zip(self.weights, new_weights)]\n",
    "        self.biases = [bias - (learningRate / len(mini_batch) * new_biases) for bias, new_biases in zip(self.biases, new_biases)]\n",
    "        \n",
    "        \n",
    "    def backprop(self, x, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function. \n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array\n",
    "            y (np.ndarray): Expected output array\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #feed forward\n",
    "        activation = x\n",
    "        activations = [x] #To store all activations in each layer\n",
    "        zs = [] #To store all z vectors\n",
    "        \n",
    "        # Iterate across layers\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation) + bias\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backwards pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * (1 / np.cosh(zs[-1])) ** 2 # Find the cost of our result\n",
    "        new_biases[-1] = delta\n",
    "        new_weights[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for layer in range(-2, -len(self.sizes), -1): # Using negative indexing\n",
    "            z = zs[layer]\n",
    "            sp = np.tanh(z)\n",
    "            delta = np.dot(self.weights[layer + 1].transpose(), delta) * sp\n",
    "            new_biases[layer] = delta\n",
    "            new_weights[layer] = np.dot(delta, activations[layer - 1].transpose())\n",
    "            \n",
    "        return (new_biases, new_weights)\n",
    "    \n",
    "    def evaluate(self, test_data : list[tuple[np.ndarray, int]]):\n",
    "        \"\"\"\n",
    "        Return the number of correct outputs\n",
    "\n",
    "        Args:\n",
    "            test_data (list[tuple[np.ndarray, np.ndarray]]): test data in the form of ([input], output)\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedForward(x)), y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return vector of partial derivatives of cost with respect to layer for the \n",
    "\n",
    "        Args:\n",
    "            output_activations (_type_): what we got\n",
    "            y (_type_): what the goal was\n",
    "        \"\"\"\n",
    "        return output_activations - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,10) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mlen\u001b[39m(myData)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmyNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestData\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 48\u001b[0m, in \u001b[0;36mNetwork.SGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, learningRate, test_data)\u001b[0m\n\u001b[0;32m     45\u001b[0m mini_batches \u001b[38;5;241m=\u001b[39m [training_data[j:j \u001b[38;5;241m+\u001b[39m mini_batch_size] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(training_data), mini_batch_size)]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_data:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "Cell \u001b[1;32mIn[36], line 69\u001b[0m, in \u001b[0;36mNetwork.update_mini_batch\u001b[1;34m(self, mini_batch, learningRate)\u001b[0m\n\u001b[0;32m     66\u001b[0m new_weights \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(w\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights]\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m mini_batch:\n\u001b[1;32m---> 69\u001b[0m     change_new_biases, change_new_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     new_biases \u001b[38;5;241m=\u001b[39m [cur \u001b[38;5;241m+\u001b[39m change \u001b[38;5;28;01mfor\u001b[39;00m cur, change \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_biases, change_new_biases)]\n\u001b[0;32m     71\u001b[0m     new_weights \u001b[38;5;241m=\u001b[39m [cur \u001b[38;5;241m+\u001b[39m change \u001b[38;5;28;01mfor\u001b[39;00m cur, change \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_weights, change_new_weights)]\n",
      "Cell \u001b[1;32mIn[36], line 101\u001b[0m, in \u001b[0;36mNetwork.backprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     98\u001b[0m     activations\u001b[38;5;241m.\u001b[39mappend(activation)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# backwards pass\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcost_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mcosh(zs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Find the cost of our result\u001b[39;00m\n\u001b[0;32m    102\u001b[0m new_biases[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m delta\n\u001b[0;32m    103\u001b[0m new_weights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, activations[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose())\n",
      "Cell \u001b[1;32mIn[36], line 133\u001b[0m, in \u001b[0;36mNetwork.cost_derivative\u001b[1;34m(self, output_activations, y)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_derivative\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_activations, y):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    Return vector of partial derivatives of cost with respect to layer for the \u001b[39;00m\n\u001b[0;32m    128\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m        y (_type_): what the goal was\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput_activations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,10) (4,) "
     ]
    }
   ],
   "source": [
    "# Lets test the network!\n",
    "myNet = Network([4, 10, 10, 4]) # have 4 neurons, 4 neurons, 2 neurons.\n",
    "\n",
    "# Here is my rule: numbers must be in order. Output should be how many numbers are in order by the end. Simple O(n) \n",
    "def numsInPlace(nums : tuple[int]) -> int:\n",
    "    return sum([1 if i + 1 == nums[i] else 0 for i in range(len(nums))])\n",
    "\n",
    "assert(numsInPlace(tuple([1, 3, 2, 4])) == 2)\n",
    "\n",
    "myData = [(np.array(x), np.array([1 if numsInPlace(x) == i else 0 for i in range(4)])) for x in list(permutations([1, 2, 3, 4], 4))]\n",
    "testData = [(np.array(x), numsInPlace(x) - 1) for x in list(permutations([1, 2, 3, 4], 4)) if random.random() < 0.3]\n",
    "myData = myData\n",
    "#self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None\n",
    "len(myData)\n",
    "myNet.SGD(myData, 5, 5, 1, test_data=testData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2.58864509, -5.83889952, -6.74733774, -7.08900456],\n",
       "        [-0.66149948, -7.95813323, -5.96480889, -5.95328965],\n",
       "        [ 0.42798111, -6.48215369, -5.00100901, -7.44021706],\n",
       "        [ 0.56384745, -6.39946471, -6.21058701, -6.63050599]]),\n",
       " array([[-2.31627021, -0.83700805, -0.27924348, -0.27206301],\n",
       "        [-1.65891698, -1.1965343 , -0.50210456, -0.74455794],\n",
       "        [-0.01145002, -1.60504269,  1.48812526,  0.5795122 ],\n",
       "        [-1.73423454, -1.54485724, -0.19229157, -0.12170709]])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
