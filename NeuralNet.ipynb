{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "A significant proportion of this project was inspired by videos from 3Blue1Brown. Along with that, this will come heavily inspired from Neural Networks and Deep Learning, the free online book. It can be reached [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/). This version is outdated so many components were changed into how I conceptually understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"\n",
    "    Creates a Neural Network in specified layers with implemented Neural Network Methods\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes : list[int]):\n",
    "        \"\"\"\n",
    "        Initializes the Network with random, normally distributed, biases and weights.\n",
    "\n",
    "        Args:\n",
    "            sizes (list): List of the number of neurons in each layer\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "        self.weights = [np.random.randn(layer, prev) for prev, layer in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def feedForward(self, inputs : list[np.ndarray]):\n",
    "        \"\"\"\n",
    "        Returns the output of the network given the input list\n",
    "\n",
    "        Args:\n",
    "            inputs (list): Input to the lyaer\n",
    "        \"\"\"\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            inputs = np.tanh(np.dot(weight, inputs) + bias)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None) -> None:\n",
    "        \"\"\"\n",
    "        Trains the network using Stoachastic Gradient Descent. \n",
    "\n",
    "        Args:\n",
    "            training_data (list of tuples): Training Data for the network\n",
    "            epochs (int): number of iterations to adjust network\n",
    "            mini_batch_size (int): size of each batch to pass through\n",
    "            learningRate (float): Amount to shift layers in backpropogration\n",
    "            test_data (list, optional): Passed through test data. Defaults to None.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[j:j + mini_batch_size] for j in range(0, len(training_data), mini_batch_size)]\n",
    "            mini_batches = [batch for batch in mini_batches if len(batch) == mini_batch_size]\n",
    "            \n",
    "            for batch in mini_batches:\n",
    "                self.update_mini_batch(batch, learningRate)\n",
    "                \n",
    "            if test_data:\n",
    "                print(f'Epoch {i + 1}: {self.evaluate(test_data)} / {len(test_data)}') \n",
    "            else:\n",
    "                print(f'Epoch {i + 1} complete!')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def update_mini_batch(self, mini_batch : list[tuple[np.ndarray, np.ndarray]], learningRate : float):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using SGD and Back Propogation\n",
    "\n",
    "        Args:\n",
    "            mini_batch (list[tuple[np.ndarray, np.ndarray]]): list containing a subset of the training data\n",
    "            learningRate (float): Amount by which to shift existing layers after backpropogration\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            change_new_biases, change_new_weights = self.backprop(x, y)\n",
    "            new_biases = [cur + change for cur, change in zip(new_biases, change_new_biases)]\n",
    "            new_weights = [cur + change for cur, change in zip(new_weights, change_new_weights)]\n",
    "        \n",
    "        self.weights = [weight - (learningRate / len(mini_batch) * new_weight) for weight, new_weight in zip(self.weights, new_weights)]\n",
    "        self.biases = [bias - (learningRate / len(mini_batch) * new_biases) for bias, new_biases in zip(self.biases, new_biases)] \n",
    "        \n",
    "        \n",
    "    def backprop(self, x, y) -> tuple:\n",
    "        \"\"\"\n",
    "        Return a tuple representing the gradient for the cost function. \n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array\n",
    "            y (np.ndarray): Expected output array\n",
    "        \"\"\"\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases] #array of zeros w/ same dimension [( 1, 2), (3, 4)] -> [(0, 0), (0, 0)] \n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #feed forward\n",
    "        activation = x\n",
    "        activations = [x] #To store all activations in each layer\n",
    "        zs = [] #To store all z vectors\n",
    "        \n",
    "        # Iterate across layers\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            z = np.dot(weight, activation).reshape(-1, 1) + bias # Have to reshape to correct dimensionality\n",
    "            zs.append(z)\n",
    "            activation = np.tanh(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # backwards pass\n",
    "        delta = self.cost_derivative(activations[-1].transpose()[0], y).reshape(-1, 1) * (1 / np.cosh(zs[-1])) ** 2 # Find the cost of our result (sech^2 from the derivative of tanh)\n",
    "        \n",
    "        new_biases[-1] = delta\n",
    "        new_weights[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for layer in range(-2, -len(self.sizes), -1): # Using negative indexing\n",
    "            z = zs[layer]\n",
    "            sp = np.tanh(z)\n",
    "            delta = np.dot(self.weights[layer + 1].transpose(), delta) * sp\n",
    "            new_biases[layer] = delta\n",
    "            if len(self.sizes) == 3: #Strange bug I encountered\n",
    "                new_weights[layer] = np.dot(delta, [activations[layer - 1]])\n",
    "            else:\n",
    "                new_weights[layer] = np.dot(delta, activations[layer - 1].T)\n",
    "            break\n",
    "            \n",
    "        return (new_biases, new_weights)\n",
    "    \n",
    "    def evaluate(self, test_data : list[tuple[np.ndarray, int]]):\n",
    "        \"\"\"\n",
    "        Return the number of correct outputs\n",
    "\n",
    "        Args:\n",
    "            test_data (list[tuple[np.ndarray, np.ndarray]]): test data in the form of ([input], output)\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedForward(x)), y) for (x,y) in test_data]\n",
    "        return sum(int(x + 0.01 > y and x - 0.01 < y) for (x, y) in test_results)\n",
    "        \n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return vector of partial derivatives of cost with respect to layer for the \n",
    "\n",
    "        Args:\n",
    "            output_activations (_type_): what we got\n",
    "            y (_type_): what the goal was\n",
    "        \"\"\"\n",
    "        return output_activations - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 48 / 100\n",
      "Epoch 2: 0 / 100\n",
      "Epoch 3: 0 / 100\n",
      "Epoch 4: 0 / 100\n",
      "Epoch 5: 86 / 100\n",
      "Epoch 6: 0 / 100\n",
      "Epoch 7: 0 / 100\n",
      "Epoch 8: 82 / 100\n",
      "Epoch 9: 82 / 100\n",
      "Epoch 10: 82 / 100\n",
      "Epoch 11: 82 / 100\n",
      "Epoch 12: 82 / 100\n",
      "Epoch 13: 82 / 100\n",
      "Epoch 14: 0 / 100\n",
      "Epoch 15: 0 / 100\n",
      "Epoch 16: 0 / 100\n",
      "Epoch 17: 0 / 100\n",
      "Epoch 18: 0 / 100\n",
      "Epoch 19: 0 / 100\n",
      "Epoch 20: 0 / 100\n",
      "Epoch 21: 0 / 100\n",
      "Epoch 22: 0 / 100\n",
      "Epoch 23: 0 / 100\n",
      "Epoch 24: 0 / 100\n",
      "Epoch 25: 0 / 100\n",
      "Epoch 26: 0 / 100\n",
      "Epoch 27: 0 / 100\n",
      "Epoch 28: 0 / 100\n",
      "Epoch 29: 0 / 100\n",
      "Epoch 30: 0 / 100\n",
      "Epoch 31: 0 / 100\n",
      "Epoch 32: 0 / 100\n",
      "Epoch 33: 0 / 100\n",
      "Epoch 34: 0 / 100\n",
      "Epoch 35: 0 / 100\n",
      "Epoch 36: 0 / 100\n",
      "Epoch 37: 0 / 100\n",
      "Epoch 38: 0 / 100\n",
      "Epoch 39: 0 / 100\n",
      "Epoch 40: 0 / 100\n",
      "Epoch 41: 0 / 100\n",
      "Epoch 42: 78 / 100\n",
      "Epoch 43: 0 / 100\n",
      "Epoch 44: 78 / 100\n",
      "Epoch 45: 0 / 100\n",
      "Epoch 46: 78 / 100\n",
      "Epoch 47: 0 / 100\n",
      "Epoch 48: 0 / 100\n",
      "Epoch 49: 78 / 100\n",
      "Epoch 50: 78 / 100\n",
      "Epoch 51: 78 / 100\n",
      "Epoch 52: 78 / 100\n",
      "Epoch 53: 78 / 100\n",
      "Epoch 54: 78 / 100\n",
      "Epoch 55: 78 / 100\n",
      "Epoch 56: 78 / 100\n",
      "Epoch 57: 78 / 100\n",
      "Epoch 58: 0 / 100\n",
      "Epoch 59: 0 / 100\n",
      "Epoch 60: 0 / 100\n",
      "Epoch 61: 0 / 100\n",
      "Epoch 62: 0 / 100\n",
      "Epoch 63: 0 / 100\n",
      "Epoch 64: 0 / 100\n",
      "Epoch 65: 0 / 100\n",
      "Epoch 66: 0 / 100\n",
      "Epoch 67: 0 / 100\n",
      "Epoch 68: 0 / 100\n",
      "Epoch 69: 0 / 100\n",
      "Epoch 70: 0 / 100\n",
      "Epoch 71: 0 / 100\n",
      "Epoch 72: 0 / 100\n",
      "Epoch 73: 0 / 100\n",
      "Epoch 74: 0 / 100\n",
      "Epoch 75: 78 / 100\n",
      "Epoch 76: 78 / 100\n",
      "Epoch 77: 78 / 100\n",
      "Epoch 78: 78 / 100\n",
      "Epoch 79: 78 / 100\n",
      "Epoch 80: 78 / 100\n",
      "Epoch 81: 78 / 100\n",
      "Epoch 82: 78 / 100\n",
      "Epoch 83: 78 / 100\n",
      "Epoch 84: 78 / 100\n",
      "Epoch 85: 78 / 100\n",
      "Epoch 86: 78 / 100\n",
      "Epoch 87: 78 / 100\n",
      "Epoch 88: 78 / 100\n",
      "Epoch 89: 78 / 100\n",
      "Epoch 90: 78 / 100\n",
      "Epoch 91: 78 / 100\n",
      "Epoch 92: 78 / 100\n",
      "Epoch 93: 78 / 100\n",
      "Epoch 94: 78 / 100\n",
      "Epoch 95: 78 / 100\n",
      "Epoch 96: 78 / 100\n",
      "Epoch 97: 78 / 100\n",
      "Epoch 98: 78 / 100\n",
      "Epoch 99: 78 / 100\n",
      "Epoch 100: 78 / 100\n"
     ]
    }
   ],
   "source": [
    "# Lets test the network!\n",
    "myNet = Network([20, 10, 4, 2]) # have 2 neurons, 3 neurons, 2 neurons.\n",
    "\n",
    "def createTestData(nums : tuple[int]):\n",
    "    if 1 in nums:\n",
    "        return [1, 0]\n",
    "    return [0, 1]\n",
    "\n",
    "def convertToInputs1(lst: list) -> list:\n",
    "    retList = [0] * 20\n",
    "    for i in range(len(lst)):\n",
    "        retList[10*i + lst[i]] = 1\n",
    "    return retList\n",
    "\n",
    "assert(convertToInputs1([1, 9]) == [0, 1, 0,0,0,0,0,0,0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "myData = [(convertToInputs1(x), np.array(createTestData(x))) for x in [(np.array([int(10 * random.random()), int(10 * random.random())])) for _ in range(10000)]]\n",
    "testData = [(convertToInputs1(x), 1 if 1 in x else 0) for x in [(np.array([int(10 * random.random()), int(10 * random.random())])) for _ in range(100)]]\n",
    "\n",
    "myNet.SGD(myData, epochs=100, mini_batch_size=30, learningRate=2, test_data=testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0 / 11\n",
      "Epoch 2: 0 / 11\n",
      "Epoch 3: 0 / 11\n",
      "Epoch 4: 0 / 11\n",
      "Epoch 5: 0 / 11\n",
      "Epoch 6: 0 / 11\n",
      "Epoch 7: 0 / 11\n",
      "Epoch 8: 0 / 11\n",
      "Epoch 9: 0 / 11\n",
      "Epoch 10: 0 / 11\n",
      "Epoch 11: 0 / 11\n",
      "Epoch 12: 0 / 11\n",
      "Epoch 13: 0 / 11\n",
      "Epoch 14: 0 / 11\n",
      "Epoch 15: 0 / 11\n",
      "Epoch 16: 0 / 11\n",
      "Epoch 17: 0 / 11\n",
      "Epoch 18: 0 / 11\n",
      "Epoch 19: 0 / 11\n",
      "Epoch 20: 0 / 11\n",
      "Epoch 21: 0 / 11\n",
      "Epoch 22: 0 / 11\n",
      "Epoch 23: 0 / 11\n",
      "Epoch 24: 0 / 11\n",
      "Epoch 25: 0 / 11\n",
      "Epoch 26: 0 / 11\n",
      "Epoch 27: 0 / 11\n",
      "Epoch 28: 0 / 11\n",
      "Epoch 29: 0 / 11\n",
      "Epoch 30: 0 / 11\n",
      "Epoch 31: 0 / 11\n",
      "Epoch 32: 0 / 11\n",
      "Epoch 33: 0 / 11\n",
      "Epoch 34: 0 / 11\n",
      "Epoch 35: 0 / 11\n",
      "Epoch 36: 0 / 11\n",
      "Epoch 37: 0 / 11\n",
      "Epoch 38: 0 / 11\n",
      "Epoch 39: 0 / 11\n",
      "Epoch 40: 0 / 11\n",
      "Epoch 41: 0 / 11\n",
      "Epoch 42: 0 / 11\n",
      "Epoch 43: 0 / 11\n",
      "Epoch 44: 0 / 11\n",
      "Epoch 45: 0 / 11\n",
      "Epoch 46: 0 / 11\n",
      "Epoch 47: 0 / 11\n",
      "Epoch 48: 0 / 11\n",
      "Epoch 49: 0 / 11\n",
      "Epoch 50: 0 / 11\n",
      "Epoch 51: 0 / 11\n",
      "Epoch 52: 0 / 11\n",
      "Epoch 53: 0 / 11\n",
      "Epoch 54: 0 / 11\n",
      "Epoch 55: 0 / 11\n",
      "Epoch 56: 0 / 11\n",
      "Epoch 57: 0 / 11\n",
      "Epoch 58: 0 / 11\n",
      "Epoch 59: 0 / 11\n",
      "Epoch 60: 0 / 11\n",
      "Epoch 61: 0 / 11\n",
      "Epoch 62: 0 / 11\n",
      "Epoch 63: 0 / 11\n",
      "Epoch 64: 0 / 11\n",
      "Epoch 65: 0 / 11\n",
      "Epoch 66: 0 / 11\n",
      "Epoch 67: 0 / 11\n",
      "Epoch 68: 0 / 11\n",
      "Epoch 69: 0 / 11\n",
      "Epoch 70: 0 / 11\n",
      "Epoch 71: 0 / 11\n",
      "Epoch 72: 0 / 11\n",
      "Epoch 73: 0 / 11\n",
      "Epoch 74: 0 / 11\n",
      "Epoch 75: 0 / 11\n",
      "Epoch 76: 0 / 11\n",
      "Epoch 77: 0 / 11\n",
      "Epoch 78: 0 / 11\n",
      "Epoch 79: 0 / 11\n",
      "Epoch 80: 0 / 11\n",
      "Epoch 81: 0 / 11\n",
      "Epoch 82: 0 / 11\n",
      "Epoch 83: 0 / 11\n",
      "Epoch 84: 0 / 11\n",
      "Epoch 85: 0 / 11\n",
      "Epoch 86: 0 / 11\n",
      "Epoch 87: 0 / 11\n",
      "Epoch 88: 0 / 11\n",
      "Epoch 89: 0 / 11\n",
      "Epoch 90: 0 / 11\n",
      "Epoch 91: 0 / 11\n",
      "Epoch 92: 0 / 11\n",
      "Epoch 93: 0 / 11\n",
      "Epoch 94: 0 / 11\n",
      "Epoch 95: 0 / 11\n",
      "Epoch 96: 0 / 11\n",
      "Epoch 97: 0 / 11\n",
      "Epoch 98: 0 / 11\n",
      "Epoch 99: 0 / 11\n",
      "Epoch 100: 0 / 11\n"
     ]
    }
   ],
   "source": [
    "# Lets test the network!\n",
    "myNet = Network([16, 32, 16, 8, 4]) # have 4 neurons, 4 neurons, 2 neurons.\n",
    "\n",
    "# Here is my rule: numbers must be in order. Output should be how many numbers are in order by the end. Simple O(n) \n",
    "def numsInPlace(nums : tuple[int]) -> int:\n",
    "    return sum([1 if i + 1 == nums[i] else 0 for i in range(len(nums))])\n",
    "\n",
    "assert(numsInPlace(tuple([1, 3, 2, 4])) == 2)\n",
    "\n",
    "def convertToInputs2(lst: list) -> list:\n",
    "    retList = [0] * 16\n",
    "    for i in range(len(lst)):\n",
    "        retList[4*i + lst[i] - 1] = 1\n",
    "    return retList\n",
    "\n",
    "assert(convertToInputs2([1, 2, 3, 4]) == [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1])\n",
    "\n",
    "myData = [(np.array(convertToInputs2(x)), np.array([1 if numsInPlace(x) == i else 0 for i in range(4)])) for x in list(permutations([1, 2, 3, 4], 4))]\n",
    "testData = [(np.array(convertToInputs2(x)), numsInPlace(convertToInputs2(x)) - 1) for x in list(permutations([1, 2, 3, 4], 4)) if random.random() < (1/3)]\n",
    "# #self, training_data : list[tuple[np.ndarray, np.ndarray]], epochs : int, mini_batch_size : int, learningRate : float, test_data : list[tuple[np.ndarray, int]] =None\n",
    "myNet.SGD(myData, epochs=100, mini_batch_size=10, learningRate=1, test_data=testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "It appears that this Neural Net doesn't effectively predict these patterns. I'm still curious about it's capabilities: thus, I plan to use it in some of my future projects. Perhaps using different inputs that are outside of just numbers would do a better job. The model was originally designed to predict handwritten numbers as according to the book. I'm sure it can be reworked using more inputs and an adjusted learning rate to be more accurate in application.\n",
    "\n",
    "I found that using more standardized inputs ([1, 0, 0, 0, 0] rather than [5]) allowed the network to perform better. I'm still curious about it's capabilities and interested to see how it can be applied. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
